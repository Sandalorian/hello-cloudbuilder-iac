filter {
  grok {
    match => {
      "[log][file][path]" => "%{GREEDYDATA}/data/%{GREEDYDATA:[neo4j][ticket_number]}/%{GREEDYDATA:[neo4j][hostname]}/metrics/%{GREEDYDATA:[neo4j][metrics][full_metric_name]}\.csv"
    }
  }

  # Process histogram data
  if [neo4j][metrics][full_metric_name] =~ "^.*db\.query\.execution\.latency\.millis" {
    csv {
      autogenerate_column_names => false
      columns => ["t","count","max","mean","min","stddev","p50","p75","p95","p98","p99","p999"]
      # count does not mean anything in our implementation
      # max, min does make any sense as they are over the whole histogram
      # p50, p75, p95 does appear to be working correctly as they are always zero
      remove_field => ["count", "max", "min","p50","p75","p95"]
      convert => {
        "mean" => "float"
        "stddev" => "float"
        "p98" => "float"
        "p99" => "float"
        "p999" => "float"
      }
      add_tag => ["histogram"]
    }
    mutate {
      rename => {
        "mean" => "[neo4j][metrics][mean]"
        "stddev" => "[neo4j][metrics][stddev]"
        "p98" => "[neo4j][metrics][p98]"
        "p99" => "[neo4j][metrics][p99]"
        "p999" => "[neo4j][metrics][p999]"
      }
    }
  }
  else {
    csv {
      autogenerate_column_names => false
      columns => ["t","value"]
    }

    mutate {
      convert => {
        "value" => "integer"
      }
    }
    mutate {
      rename => {
        "value" => "[neo4j][metrics][value]"
      }
    }
  }


  # All metrics files have column t which is epoch of the event
  date {
    match => ["[t]","UNIX"]
  }
  mutate {
    remove_field => ["t"]
  }

  # if we failed to parse a date from the first column, something is wrong with the format (could be a header line)
  # the rest of the data will not parse correctly so drop the event
  if "_dateparsefailure" in [tags] {
    drop {
    }
  }

  # Parse the metric "short" name and it's database.
  # This will need to take into account if metrics.namespace.enabled behaviour as describe
  # in this link: https://neo4j.com/docs/operations-manual/5/monitoring/metrics/reference/#metrics-global
  # We make an assumption there is no full stop in the database name and metrics.prefix and consequently in <database-name> and <user-configured-prefix>. If there is, this will break the parsing logic. 
  if ".dbms." in [neo4j][metrics][full_metric_name] or ".database." in [neo4j][metrics][full_metric_name] {
    # This is a metric with metrics.namespace.enabled=true
    # We can parse this using <user-configured-prefix>.dbms.<metric-name> and <user-configured-prefix>.database.<database-name>.<metric-name>
    # We can skip field creation for <user-configured-prefix>, "dbms" and "database" as this is not needed.
    grok {
      match => {
        "[neo4j][metrics][full_metric_name]" => "([^.]*)(\.database\.|\.dbms\.)(?<db_name>([^.]*)).%{GREEDYDATA:[neo4j][metrics][metric_name]}"
      }
    }
  }
  else {
    # This is a database metric with metrics.namespace.enabled=false and there is no namespacing
    # We can parse this using <user-configured-prefix>.<database_name>.<metric-name>
    # We can skip field creation for  <user-configured-prefix> as this is not needed 
    grok {
      match => {
        "[neo4j][metrics][full_metric_name]" => "([^.]*).(?<db_name>([^.]*)).%{GREEDYDATA:[neo4j][metrics][metric_name]}"
      }
    }
  }

  # required as nested fields are not supported in regex expressions
  # https://github.com/logstash-plugins/logstash-filter-grok/issues/66
  mutate {
    rename => {
      "db_name" => "[neo4j][metrics][database_name]"
    }
  }

  #Fix for not real db_name
  if [neo4j][metrics][database_name] == "bolt" or [neo4j][metrics][database_name] == "vm" or [neo4j][metrics][database_name] == "page_cache" or [neo4j][metrics][database_name] == "db" {
    # if [neo4j][metrics][database_name] == "bolt" { 
    mutate {
      update => {
        "[neo4j][metrics][metric_name]" => "%{[neo4j][metrics][database_name]}.%{[neo4j][metrics][metric_name]}"
      }
      remove_field => [ "[neo4j][metrics][database_name]" ]
    }
  }


  # required as per: https://github.com/elastic/ecs/issues/35
  # This GH issue seems to affect the file input plugin also
  mutate {
    remove_field => [ "host" ]
  }

  # added for ECS support
  mutate {
    replace => {
      "[event][dataset]" => "metrics"
    }
  }
}
input {
  file {
    path => ["/home/logstash/data/**/**/metrics/*.csv*"]
    exclude => ["*.zip","*.tgz","*.gz"] 
    mode => "read"
    exit_after_read => true
    file_completed_action => log
    # must be set when file_completed_action is log
    file_completed_log_path => "/usr/share/logstash/metrics_file.log"
    # uncomment for dev purposes
    #sincedb_path => "/dev/null"
    start_position => "beginning"
    tags => ["metrics"]
  }
} 
output {
  elasticsearch {
    hosts => [ "${ECK_ES_HOSTS}" ]
    user => "${ECK_ES_USER}"
    password => "${ECK_ES_PASSWORD}"
    ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
    data_stream => "true"
    data_stream_namespace => "synlig"
  }
}